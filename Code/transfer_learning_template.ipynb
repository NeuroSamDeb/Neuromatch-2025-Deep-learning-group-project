{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "7ZzvYU1g12ne"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ [Jama Hussein Mohamud](https://engmubarak48.github.io/jmohamud/index.html) & [Alex Hernandez-Garcia](https://alexhernandezgarcia.github.io/)\n",
        "\n",
        "__Production editors:__ Saeed Salehi, Spiros Chavlis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "x4UH_-lV12nh"
      },
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "One desired capability for machines is the ability to transfer the knowledge (features) learned on one domain to another This can potentially save compute time, enable training when data is scarce, and even improve performance. Unfortunately, there is no single recipe for transfer learning and instead multiple options are possible and much remains to be well understood. In this project, you will explore how transfer learning works in different scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "DZuAH75712ni"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "v8PBwdlc12nk"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import gc\n",
        "import csv\n",
        "import glob\n",
        "import torch\n",
        "import multiprocessing\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "PBAxxhor12nm"
      },
      "outputs": [],
      "source": [
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# for DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "9_5S49IG12no"
      },
      "outputs": [],
      "source": [
        "# @title Set device (GPU or CPU)\n",
        "\n",
        "# inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "9wAAhwKU12nq"
      },
      "source": [
        "### Random seeds\n",
        "\n",
        "If you want to obtain reproducible results, it is a good practice to set seeds for the random number generators of the various libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {},
        "id": "f_gx5gBc12nr",
        "outputId": "ffb086d9-0e84-4143-f994-2b0ce3d6ead7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2021 has been set.\n",
            "GPU is enabled in this notebook.\n"
          ]
        }
      ],
      "source": [
        "set_seed(seed=2021)\n",
        "device = set_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "VTJ39hiq12nt"
      },
      "source": [
        "### Training hyperparameters\n",
        "\n",
        "Here we set some general training hyperparameters such as the learning rate, batch size, etc. as well as other training options such as including data augmentation (`torchvision_transforms`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {},
        "id": "lD4-f24t12nw"
      },
      "outputs": [],
      "source": [
        "# hyper-parameters\n",
        "use_cuda = torch.cuda.is_available()\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "batch_size = 128\n",
        "max_epochs = 15  # Please change this to 200\n",
        "max_epochs_target = 10\n",
        "base_learning_rate = 0.1\n",
        "torchvision_transforms = True  # True/False if you want use torchvision augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "AIkihoQs12nx"
      },
      "source": [
        "---\n",
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "u26E7ODL12nx"
      },
      "source": [
        "## Source dataset\n",
        "\n",
        "We will train the source model using CIFAR-100 data set from PyTorch, but with small tweaks we can get any other data we are interested in.\n",
        "\n",
        "Note that the data set is normalised by substracted the mean and dividing by the standard deviation (pre-computed) of the training set. Also, if `torchvision_transforms` is `True`, data augmentation will be applied during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "MeIl0r7_12nz",
        "outputId": "2a9bdadb-0f75-4ce6-ecfd-ad66a0bdb597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:03<00:00, 45.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @markdown Download and prepare Data\n",
        "print('==> Preparing data..')\n",
        "def percentageSplit(full_dataset, percent = 0.0):\n",
        "  set1_size = int(percent * len(full_dataset))\n",
        "  set2_size = len(full_dataset) - set1_size\n",
        "  final_dataset, _ = torch.utils.data.random_split(full_dataset, [set1_size, set2_size])\n",
        "  return final_dataset\n",
        "\n",
        "\n",
        "# CIFAR100 normalizing\n",
        "mean = [0.5071, 0.4866, 0.4409]\n",
        "std = [0.2673, 0.2564, 0.2762]\n",
        "\n",
        "# CIFAR10 normalizing\n",
        "# mean = (0.4914, 0.4822, 0.4465)\n",
        "# std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "# torchvision transforms\n",
        "transform_train = transforms.Compose([])\n",
        "if torchvision_transforms:\n",
        "  transform_train.transforms.append(transforms.RandomCrop(32, padding=4))\n",
        "  transform_train.transforms.append(transforms.RandomHorizontalFlip())\n",
        "\n",
        "transform_train.transforms.append(transforms.ToTensor())\n",
        "transform_train.transforms.append(transforms.Normalize(mean, std))\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(\n",
        "  root='./CIFAR100', train=True, download=True, transform=transform_train)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(\n",
        "  root='./CIFAR100', train=False, download=True, transform=transform_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BxIcYCJ512n1"
      },
      "source": [
        "### CIFAR-100\n",
        "\n",
        "CIFAR-100 is a data set of 50,000 colour (RGB) training images and 10,000 test images, of size 32 x 32 pixels. Each image is labelled as 1 of 100 possible classes.\n",
        "\n",
        "The data set is stored as a custom `torchvision.datasets.cifar.CIFAR` object. You can check some of its properties with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {},
        "id": "Rwq_yXoS12n3",
        "outputId": "206f64a3-7706-4cd0-f8f8-989c45ea1543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object type: <class 'torchvision.datasets.cifar.CIFAR100'>\n",
            "Training data shape: (50000, 32, 32, 3)\n",
            "Test data shape: (10000, 32, 32, 3)\n",
            "Number of classes: 100\n"
          ]
        }
      ],
      "source": [
        "print(f\"Object type: {type(trainset)}\")\n",
        "print(f\"Training data shape: {trainset.data.shape}\")\n",
        "print(f\"Test data shape: {testset.data.shape}\")\n",
        "print(f\"Number of classes: {np.unique(trainset.targets).shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "uAYab1AC12n3"
      },
      "source": [
        "## Data loaders\n",
        "\n",
        "A dataloader is an optimized data iterator that provides functionality for efficient shuffling, transformation and batching of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "BAeN-seV12n4",
        "outputId": "dcdd5a38-e529-4f67-d4e6-656b2e783ec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----> number of workers: 2\n"
          ]
        }
      ],
      "source": [
        "##@title Dataloader\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "\n",
        "print(f'----> number of workers: {num_workers}')\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "JVawZReQ12n7"
      },
      "source": [
        "## Architecture: ResNet\n",
        "\n",
        "ResNet is a family of network architectures whose main property is that the network is organised as a stack of _residual blocks_. Residual blocks consist of a stack of layers whose output is added the input, making a _shortcut connection_.\n",
        "\n",
        "See the [original paper](https://arxiv.org/abs/1512.03385) for more details.\n",
        "\n",
        "ResNet is just a popular choice out of many others, but data augmentation works well in general. We just picked ResNet for illustration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "kCe5mgqp12n7"
      },
      "outputs": [],
      "source": [
        "# @title ResNet model in PyTorch\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  \"\"\"ResNet in PyTorch.\n",
        "      Reference:\n",
        "      [1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "        Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "  \"\"\"\n",
        "\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes:\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(self.expansion*planes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion*planes:\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(self.expansion*planes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = F.relu(self.bn2(self.conv2(out)))\n",
        "    out = self.bn3(self.conv3(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_classes=100):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 64\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "    self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes * block.expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = F.avg_pool2d(out, 4)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "  return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "  return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "  return ResNet(Bottleneck, [3, 4, 6, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rMkWgFzj12n-"
      },
      "source": [
        "#### Test on random data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {},
        "id": "F6O7Pwqz12n_",
        "outputId": "94c57eda-069c-4736-9ef2-58d045110167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----> verify if model is run on random data\n",
            "model loaded\n",
            "Using 1 GPUs.\n",
            "Using CUDA..\n"
          ]
        }
      ],
      "source": [
        "# Load the Model\n",
        "net = ResNet18()\n",
        "print('-----> verify if model is run on random data')\n",
        "y = net(Variable(torch.randn(1,3,32,32)))\n",
        "print('model loaded')\n",
        "\n",
        "result_folder = './results/'\n",
        "if not os.path.exists(result_folder):\n",
        "    os.makedirs(result_folder)\n",
        "\n",
        "logname = result_folder + net.__class__.__name__ + '_pretrain' + '.csv'\n",
        "\n",
        "if use_cuda:\n",
        "  net.cuda()\n",
        "  net = torch.nn.DataParallel(net)\n",
        "  print('Using', torch.cuda.device_count(), 'GPUs.')\n",
        "  cudnn.benchmark = True\n",
        "  print('Using CUDA..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "teTAZNBe12oA"
      },
      "source": [
        "## Set up training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "4lAA2-Z_12oB"
      },
      "source": [
        "### Set loss function and optimizer\n",
        "\n",
        "We use the cross entropy loss, commonly used for classification, and stochastic gradient descent (SGD) as optimizer, with momentum and weight decay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {},
        "id": "etE7EoCg12oC"
      },
      "outputs": [],
      "source": [
        "# Optimizer and criterion\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=base_learning_rate, momentum=0.9, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "71U-DBQL12oD"
      },
      "source": [
        "### Train and test loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {},
        "id": "aJAGTEKk12oE"
      },
      "outputs": [],
      "source": [
        "# Training & Test functions\n",
        "\n",
        "def train(net, epoch, use_cuda=True):\n",
        "  print('\\nEpoch: %d' % epoch)\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "    if use_cuda:\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    inputs, targets = Variable(inputs), Variable(targets)\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += targets.size(0)\n",
        "    correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "    if batch_idx % 500 == 0:\n",
        "      print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "          % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "  return (train_loss/batch_idx, 100.*correct/total)\n",
        "\n",
        "\n",
        "def test(net, epoch, outModelName, use_cuda=True):\n",
        "  global best_acc\n",
        "  net.eval()\n",
        "  test_loss, correct, total = 0, 0, 0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "      if use_cuda:\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = net(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += targets.size(0)\n",
        "      correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "      if batch_idx % 200 == 0:\n",
        "        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "  # Save checkpoint.\n",
        "  acc = 100.*correct/total\n",
        "  if acc > best_acc:\n",
        "    best_acc = acc\n",
        "    checkpoint(net, acc, epoch, outModelName)\n",
        "  return (test_loss/batch_idx, 100.*correct/total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "GHPnDFjp12oF"
      },
      "source": [
        "### Auxiliary functions\n",
        "\n",
        "* `checkpoint()`: Store checkpoints of the model\n",
        "* `adjust_learning_rate()`: Decreases the learning rate (learning rate decay) at certain epochs of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {},
        "id": "j8Z_RDlH12oF"
      },
      "outputs": [],
      "source": [
        "# checkpoint & adjust_learning_rate\n",
        "def checkpoint(model, acc, epoch, outModelName):\n",
        "  # Save checkpoint.\n",
        "  print('Saving..')\n",
        "  state = {\n",
        "      'state_dict': model.state_dict(),\n",
        "      'acc': acc,\n",
        "      'epoch': epoch,\n",
        "      'rng_state': torch.get_rng_state()\n",
        "  }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "      os.mkdir('checkpoint')\n",
        "  torch.save(state, f'./checkpoint/{outModelName}.t7')\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "  \"\"\"decrease the learning rate at 100 and 150 epoch\"\"\"\n",
        "  lr = base_learning_rate\n",
        "  if epoch <= 9 and lr > 0.1:\n",
        "    # warm-up training for large minibatch\n",
        "    lr = 0.1 + (base_learning_rate - 0.1) * epoch / 10.\n",
        "  if epoch >= 100:\n",
        "    lr /= 10\n",
        "  if epoch >= 150:\n",
        "    lr /= 10\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Y2ET3wKJ12oG"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "This is the loop where the model is trained for `max_epochs` epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {},
        "id": "WOth7JMu12oH",
        "outputId": "443937f2-9657-411d-8a6c-247d75d64c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 4.748 | Acc: 0.781% (1/128)\n",
            "0 79 Loss: 3.531 | Acc: 12.500% (16/128)\n",
            "Saving..\n",
            "Epoch: 0 | train acc: 8.473999977111816 | test acc: 12.609999656677246\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 3.613 | Acc: 16.406% (21/128)\n",
            "0 79 Loss: 2.969 | Acc: 27.344% (35/128)\n",
            "Saving..\n",
            "Epoch: 1 | train acc: 18.384000778198242 | test acc: 23.84000015258789\n",
            "\n",
            "Epoch: 2\n",
            "0 391 Loss: 2.889 | Acc: 28.125% (36/128)\n",
            "0 79 Loss: 2.397 | Acc: 39.844% (51/128)\n",
            "Saving..\n",
            "Epoch: 2 | train acc: 28.281999588012695 | test acc: 31.950000762939453\n",
            "\n",
            "Epoch: 3\n",
            "0 391 Loss: 2.554 | Acc: 32.812% (42/128)\n",
            "0 79 Loss: 2.267 | Acc: 40.625% (52/128)\n",
            "Saving..\n",
            "Epoch: 3 | train acc: 37.15399932861328 | test acc: 38.029998779296875\n",
            "\n",
            "Epoch: 4\n",
            "0 391 Loss: 2.026 | Acc: 42.969% (55/128)\n",
            "0 79 Loss: 1.921 | Acc: 47.656% (61/128)\n",
            "Saving..\n",
            "Epoch: 4 | train acc: 44.231998443603516 | test acc: 44.439998626708984\n",
            "\n",
            "Epoch: 5\n",
            "0 391 Loss: 1.856 | Acc: 50.000% (64/128)\n",
            "0 79 Loss: 1.959 | Acc: 49.219% (63/128)\n",
            "Saving..\n",
            "Epoch: 5 | train acc: 49.805999755859375 | test acc: 47.150001525878906\n",
            "\n",
            "Epoch: 6\n",
            "0 391 Loss: 1.771 | Acc: 53.125% (68/128)\n",
            "0 79 Loss: 1.998 | Acc: 51.562% (66/128)\n",
            "Saving..\n",
            "Epoch: 6 | train acc: 54.61199951171875 | test acc: 50.310001373291016\n",
            "\n",
            "Epoch: 7\n",
            "0 391 Loss: 1.419 | Acc: 60.938% (78/128)\n",
            "0 79 Loss: 1.611 | Acc: 53.906% (69/128)\n",
            "Saving..\n",
            "Epoch: 7 | train acc: 58.305999755859375 | test acc: 55.63999938964844\n",
            "\n",
            "Epoch: 8\n",
            "0 391 Loss: 1.255 | Acc: 67.188% (86/128)\n",
            "0 79 Loss: 1.371 | Acc: 60.156% (77/128)\n",
            "Saving..\n",
            "Epoch: 8 | train acc: 61.18199920654297 | test acc: 58.5\n",
            "\n",
            "Epoch: 9\n",
            "0 391 Loss: 1.065 | Acc: 70.312% (90/128)\n",
            "0 79 Loss: 1.455 | Acc: 60.938% (78/128)\n",
            "Epoch: 9 | train acc: 64.11000061035156 | test acc: 57.779998779296875\n",
            "\n",
            "Epoch: 10\n",
            "0 391 Loss: 0.964 | Acc: 72.656% (93/128)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3821675709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutModelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlogfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3924816570.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epoch, use_cuda)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "outModelName = 'pretrain'\n",
        "if not os.path.exists(logname):\n",
        "  with open(logname, 'w') as logfile:\n",
        "      logwriter = csv.writer(logfile, delimiter=',')\n",
        "      logwriter.writerow(['epoch', 'train loss', 'train acc', 'test loss', 'test acc'])\n",
        "\n",
        "for epoch in range(start_epoch, max_epochs):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train_loss, train_acc = train(net, epoch, use_cuda=use_cuda)\n",
        "  test_loss, test_acc = test(net, epoch, outModelName, use_cuda=use_cuda)\n",
        "  with open(logname, 'a') as logfile:\n",
        "    logwriter = csv.writer(logfile, delimiter=',')\n",
        "    logwriter.writerow([epoch, train_loss, train_acc.item(), test_loss, test_acc.item()])\n",
        "  print(f'Epoch: {epoch} | train acc: {train_acc} | test acc: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "GJP0Ibpa12oH"
      },
      "source": [
        "## Transfer learning\n",
        "### Re-use the trained model to improve training on a different data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dWtGmahl12oI"
      },
      "source": [
        "### Delete variables from the previous model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "a6mreKqe12oK"
      },
      "outputs": [],
      "source": [
        "# delete the backbone network\n",
        "delete = True\n",
        "if delete:\n",
        "  del net\n",
        "  del trainset\n",
        "  del testset\n",
        "  del trainloader\n",
        "  del testloader\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rbWVH2NZ12oK"
      },
      "source": [
        "#### Target dataset\n",
        "\n",
        "We will now use CIFAR-10 as _target_ data set. Again, with small tweaks we can get any other data we are interested in.\n",
        "\n",
        "CIFAR-10 is very similar to CIFAR-100, but it contains only 10 classes instead of 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "neX7rGs512oL"
      },
      "outputs": [],
      "source": [
        "# Target domain Data\n",
        "print('==> Preparing target domain data..')\n",
        "\n",
        "# CIFAR10 normalizing\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2023, 0.1994, 0.2010)\n",
        "num_classes = 10\n",
        "lr = 0.0001\n",
        "\n",
        "# torchvision transforms\n",
        "transform_train = transforms.Compose([])\n",
        "if torchvision_transforms:\n",
        "    transform_train.transforms.append(transforms.RandomCrop(32, padding=4))\n",
        "    transform_train.transforms.append(transforms.RandomHorizontalFlip())\n",
        "\n",
        "transform_train.transforms.append(transforms.ToTensor())\n",
        "transform_train.transforms.append(transforms.Normalize(mean, std))\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./CIFAR10', train=True, download=True, transform=transform_train)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./CIFAR10', train=False, download=True, transform=transform_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "aiWzGHcY12oL"
      },
      "source": [
        "#### Select a subset of the data\n",
        "\n",
        "To simulate a lower data regime, where transfer learning can be useful.\n",
        "\n",
        "Choose percentage from the trainset. Set `percent = 1.0` to use the whole train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "eh1Cjkts12oM"
      },
      "outputs": [],
      "source": [
        "percent = 0.6\n",
        "\n",
        "trainset = percentageSplit(trainset, percent = percent)\n",
        "print('size of the new trainset: ', len(trainset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "T7VuWCmD12oM"
      },
      "source": [
        "#### Dataloaders\n",
        "\n",
        "As before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "AId3aZWx12oN"
      },
      "outputs": [],
      "source": [
        "# Dataloader\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "\n",
        "print(f'----> number of workers: {num_workers}')\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "AtPfcyen12oO"
      },
      "source": [
        "### Load pre-trained model\n",
        "\n",
        "Load the checkpoint of the model previously trained on CIFAR-100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "uvRUeLWx12oO"
      },
      "outputs": [],
      "source": [
        "model = ResNet18()\n",
        "\n",
        "checkpointPath = '/content/checkpoint/pretrain.t7'\n",
        "\n",
        "print(' ===> loading pretrained model from: ', checkpointPath)\n",
        "if os.path.isfile(checkpointPath):\n",
        "  state_dict = torch.load(checkpointPath)\n",
        "  best_acc = state_dict['acc']\n",
        "  print('Best Accuracy:', best_acc)\n",
        "  if \"state_dict\" in state_dict:\n",
        "      state_dict = state_dict[\"state_dict\"]\n",
        "  # remove prefixe \"module.\"\n",
        "  state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
        "  for k, v in model.state_dict().items():\n",
        "    if k not in list(state_dict):\n",
        "      print('key \"{}\" could not be found in provided state dict'.format(k))\n",
        "    elif state_dict[k].shape != v.shape:\n",
        "      print('key \"{}\" is of different shape in model and provided state dict'.format(k))\n",
        "      state_dict[k] = v\n",
        "  msg = model.load_state_dict(state_dict, strict=False)\n",
        "  print(\"Load pretrained model with msg: {}\".format(msg))\n",
        "else:\n",
        "  raise Exception('No pretrained weights found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UUCoaxH812oP"
      },
      "source": [
        "### Freeze model parameters\n",
        "\n",
        "In transfer learning, we usually do not re-train all the weights of the model, but only a subset of them, for instance the last layer. Here we first _freeze_ all the parameters of the model, and we will _unfreeze_ one layer below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "5PbcBubn12oP"
      },
      "outputs": [],
      "source": [
        "# Freeze the model parameters, you can also freeze some layers only\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "2kWGPEIr12oQ"
      },
      "source": [
        "### Loss function, optimizer and _unfreeze_ last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "E3fjQa_n12oQ"
      },
      "outputs": [],
      "source": [
        "num_ftrs = model.linear.in_features\n",
        "model.linear = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.linear.parameters(),\n",
        "    lr=lr,\n",
        "    momentum=0.9,\n",
        "    weight_decay=1e-4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "sxedjsoh12oQ"
      },
      "source": [
        "#### Check number of parameters\n",
        "\n",
        "We can calculate the number of total parameters and the number of trainable parameters, that is those that will be updated during training. Since we have freezed most of the parameters, the number of training parameters should be much smaller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "z0eQUqis12oR"
      },
      "outputs": [],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('Total Parameters:', total_params, 'Trainable parameters: ', trainable_total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "P3dL-7vV12oR"
      },
      "source": [
        "### Train the target model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "cYeeNc1z12oS"
      },
      "outputs": [],
      "source": [
        "outModelName = 'finetuned'\n",
        "logname = result_folder + model.__class__.__name__ + f'_{outModelName}.csv'\n",
        "\n",
        "if not os.path.exists(logname):\n",
        "  with open(logname, 'w') as logfile:\n",
        "    logwriter = csv.writer(logfile, delimiter=',')\n",
        "    logwriter.writerow(['epoch', 'train loss', 'train acc', 'test loss', 'test acc'])\n",
        "\n",
        "for epoch in range(start_epoch, max_epochs_target):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train_loss, train_acc = train(model, epoch, use_cuda=use_cuda)\n",
        "  test_loss, test_acc = test(model, epoch, outModelName, use_cuda=use_cuda)\n",
        "  with open(logname, 'a') as logfile:\n",
        "    logwriter = csv.writer(logfile, delimiter=',')\n",
        "    logwriter.writerow([epoch, train_loss, train_acc.item(), test_loss, test_acc.item()])\n",
        "  print(f'Epoch: {epoch} | train acc: {train_acc} | test acc: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6zU4Z9I612oS"
      },
      "source": [
        "## Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "ryBeZj8i12oT"
      },
      "outputs": [],
      "source": [
        "# title plot results\n",
        "results = pd.read_csv(f'/content/results/ResNet_{outModelName}.csv', sep =',')\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "AFtZiQpW12oT"
      },
      "outputs": [],
      "source": [
        "train_accuracy = results['train acc'].values\n",
        "test_accuracy = results['test acc'].values\n",
        "\n",
        "print(f'Average Accuracy over {max_epochs_target} epochs:', sum(test_accuracy)//len(test_accuracy))\n",
        "print(f'best accuraccy over {max_epochs_target} epochs:', max(test_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "mblDnA4L12oU"
      },
      "outputs": [],
      "source": [
        "figureName = 'figure' # change figure name\n",
        "\n",
        "plt.plot(results['epoch'].values, train_accuracy, label='train')\n",
        "plt.plot(results['epoch'].values, test_accuracy, label='test')\n",
        "plt.xlabel('Number of epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(f'Train/Test Accuracy curve for {max_epochs} epochs')\n",
        "plt.savefig(f'/content/results/{figureName}.png')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}